{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rib0VDKT8FJg",
    "outputId": "2a5bab27-4d40-4e78-de88-42ffb1beeb43"
   },
   "outputs": [],
   "source": [
    "!pip -q install \\\n",
    "  notebook \\\n",
    "  aiohttp \\\n",
    "  faiss-cpu \\\n",
    "  \"torch>=2.2,<3.0\" \\\n",
    "  sentence-transformers \\\n",
    "  tree_sitter_python \\\n",
    "  tree_sitter \\\n",
    "  google.genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uF5K2l6c7eBy"
   },
   "outputs": [],
   "source": [
    "import httpx\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "async def fetch_github_repo_content(url: str) -> dict:\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.object\",\n",
    "        \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
    "    }\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    return {}\n",
    "\n",
    "async def fetch_file_content_from_download_url(download_url: str, client: httpx.AsyncClient, semaphore: asyncio.Semaphore) -> str:\n",
    "    async with semaphore:\n",
    "        response = await client.get(download_url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    return \"\"\n",
    "\n",
    "async def fetch_list_of_file_name_content_tuples(github_repo_content: dict, max_concurrent: int = 3):\n",
    "    python_files = [item for item in github_repo_content['entries'] if item['name'].endswith('.py')]\n",
    "    semaphore = asyncio.Semaphore(max_concurrent)\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        content_tasks = []\n",
    "        for pf in python_files:\n",
    "            content_task = fetch_file_content_from_download_url(\n",
    "                pf['download_url'],\n",
    "                client,\n",
    "                semaphore\n",
    "            )\n",
    "            content_tasks.append(content_task)\n",
    "\n",
    "        contents = await asyncio.gather(*content_tasks)\n",
    "\n",
    "    name_content_tuples = list(zip([pf['name'] for pf in python_files], contents))\n",
    "    return name_content_tuples\n",
    "\n",
    "\n",
    "def write_python_files(relative_path: str, name_content_tuples: list) -> None:\n",
    "    output_dir = Path.cwd() / relative_path\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for name, content in name_content_tuples:\n",
    "        file_path = output_dir / name\n",
    "        file_path.write_text(content, encoding='utf-8')\n",
    "\n",
    "\n",
    "async def fetch_urls_and_save_in_dirs():\n",
    "    urls = [\n",
    "        \"https://api.github.com/repos/neetcode-gh/leetcode/contents/python?ref=main\",\n",
    "        \"https://api.github.com/repos/TheAlgorithms/Python/contents/sorts?ref=master\"\n",
    "    ]\n",
    "\n",
    "    dir_names = ['neetcode', 'sorts']\n",
    "    for i in range(len(dir_names)):\n",
    "        url = urls[i]\n",
    "        dir_name = dir_names[i]\n",
    "\n",
    "        repo = await fetch_github_repo_content(url)\n",
    "        names_contents = await fetch_list_of_file_name_content_tuples(repo)\n",
    "        write_python_files(dir_name, names_contents)\n",
    "\n",
    "await fetch_urls_and_save_in_dirs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NMU4adEt7hTa"
   },
   "outputs": [],
   "source": [
    "import tree_sitter_python as tspython\n",
    "from tree_sitter import Language, Parser\n",
    "from pathlib import Path\n",
    "PY_LANGUAGE = Language(tspython.language())\n",
    "parser = Parser(PY_LANGUAGE)\n",
    "\n",
    "\n",
    "def chunk_python_code_in_functions_and_classes(code:str):\n",
    "    tree = parser.parse(bytes(code, \"utf8\"))\n",
    "    root_node = tree.root_node\n",
    "    function_node = root_node.children\n",
    "    functions_and_classes = [f for f in function_node if f.type in ['function_definition', 'class_definition']]\n",
    "    return [fc.text for fc in functions_and_classes]\n",
    "\n",
    "\n",
    "def get_chunks_of_fuctions_and_classes_from_dir(dir_path):\n",
    "    base = Path(dir_path)\n",
    "    result = []\n",
    "    for file in base.rglob(\"*\"):\n",
    "        if file.is_file() and file.suffix == '.py':\n",
    "            code = file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            chunks = chunk_python_code_in_functions_and_classes(code)\n",
    "            result.extend(chunks)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "LDTWBRYz7jrz"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.colab import userdata\n",
    "    from google.genai import types\n",
    "    google_key = userdata.get('GEMINI_API_KEY')\n",
    "except:\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    google_key = os.getenv('GEMINI_API_KEY')\n",
    "\n",
    "client = genai.Client(api_key=google_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qZUDc2Gm7k4S"
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks:list[str]):\n",
    "  result = client.models.embed_content(\n",
    "          model=\"gemini-embedding-001\",\n",
    "          contents=chunks,\n",
    "          config=types.EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "          )\n",
    "  embeddings = np.array([embedding.values for embedding in result.embeddings])\n",
    "  embeddings = embeddings.astype('float32')\n",
    "\n",
    "  return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTDxqMAI7mW6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tJqyRTiN7nia"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dim = \u001b[43membeddings\u001b[49m.shape[\u001b[32m1\u001b[39m]\n\u001b[32m      3\u001b[39m faiss.normalize_L2(embeddings)\n\u001b[32m      4\u001b[39m metric = faiss.METRIC_INNER_PRODUCT\n",
      "\u001b[31mNameError\u001b[39m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np, faiss\n",
    "dim = embeddings.shape[1]\n",
    "faiss.normalize_L2(embeddings)\n",
    "metric = faiss.METRIC_INNER_PRODUCT\n",
    "M=32\n",
    "base = faiss.IndexHNSWFlat(dim, M, metric)\n",
    "base.hnsw.efConstruction = 200\n",
    "base.hnsw.efSearch = 64\n",
    "index = faiss.IndexIDMap2(base)\n",
    "ids = [i for i in range(embeddings.shape[0])]\n",
    "ids = np.array(ids)\n",
    "index.add_with_ids(embeddings, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings, FAISS index, and chunks to disk\n",
    "import pickle\n",
    "\n",
    "# Create a directory for saved data\n",
    "save_dir = Path('saved_data')\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. Save embeddings as numpy array\n",
    "np.save(save_dir / 'embeddings.npy', embeddings)\n",
    "print(f\"Saved embeddings to {save_dir / 'embeddings.npy'}\")\n",
    "\n",
    "# 2. Save FAISS index\n",
    "faiss.write_index(index, str(save_dir / 'faiss_index.index'))\n",
    "print(f\"Saved FAISS index to {save_dir / 'faiss_index.index'}\")\n",
    "\n",
    "# 3. Save chunks (code snippets) as pickle for easy retrieval\n",
    "with open(save_dir / 'chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(chunks, f)\n",
    "print(f\"Saved chunks to {save_dir / 'chunks.pkl'}\")\n",
    "\n",
    "print(f\"\\nAll data saved to '{save_dir}' directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings, FAISS index, and chunks from disk\n",
    "load_dir = Path('saved_data')\n",
    "\n",
    "# 1. Load embeddings\n",
    "loaded_embeddings = np.load(load_dir / 'embeddings.npy')\n",
    "print(f\"Loaded embeddings shape: {loaded_embeddings.shape}\")\n",
    "\n",
    "# 2. Load FAISS index\n",
    "loaded_index = faiss.read_index(str(load_dir / 'faiss_index.index'))\n",
    "print(f\"Loaded FAISS index with {loaded_index.ntotal} vectors\")\n",
    "\n",
    "# 3. Load chunks\n",
    "with open(load_dir / 'chunks.pkl', 'rb') as f:\n",
    "    loaded_chunks = pickle.load(f)\n",
    "print(f\"Loaded {len(loaded_chunks)} chunks\")\n",
    "\n",
    "print(\"\\nAll data loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FWMzRaHZ7o0J",
    "outputId": "0f6ef1c8-ff91-467c-86dc-1f87b7a2779a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9952494  0.81578714 0.808552   0.8019581  0.792458  ]] [[77 67  0  2 33]]\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "\n",
    "def bucket_sort(my_list: list, bucket_count: int = 10) -> list:\n",
    "    \"\"\n",
    "    >>> data = [-1, 2, -5, 0]\n",
    "    >>> bucket_sort(data) == sorted(data)\n",
    "    True\n",
    "    >>> data = [9, 8, 7, 6, -12]\n",
    "    >>> bucket_sort(data) == sorted(data)\n",
    "    True\n",
    "    >>> data = [.4, 1.2, .1, .2, -.9]\n",
    "    >>> bucket_sort(data) == sorted(data)\n",
    "    True\n",
    "    >>> bucket_sort([]) == sorted([])\n",
    "    True\n",
    "   2, 2, 1, 1, 3]\n",
    "    >>> bucket_sort(data) == sorted(data)\n",
    "    True\n",
    "    >>> data = [5, 5, 5, 5, 5]\n",
    "\"\"\"\n",
    "search_chunks = [code]\n",
    "search_embed = generate_embeddings(search_chunks)\n",
    "faiss.normalize_L2(search_embed)\n",
    "D, I = index.search(search_embed, k=5)\n",
    "\n",
    "print(D,I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eg6TGAyc9Hy2",
    "outputId": "a2a3bb21-21b1-4d9e-b342-545fc8cad8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'def bucket_sort(my_list: list, bucket_count: int = 10) -> list:\\n    \"\"\"\\n    >>> data = [-1, 2, -5, 0]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> data = [9, 8, 7, 6, -12]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> data = [.4, 1.2, .1, .2, -.9]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> bucket_sort([]) == sorted([])\\n    True\\n    >>> data = [-1e10, 1e10]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> import random\\n    >>> collection = random.sample(range(-50, 50), 50)\\n    >>> bucket_sort(collection) == sorted(collection)\\n    True\\n    >>> data = [1, 2, 2, 1, 1, 3]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> data = [5, 5, 5, 5, 5]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> data = [1000, -1000, 500, -500, 0]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> data = [5.5, 2.2, -1.1, 3.3, 0.0]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> bucket_sort([1]) == [1]\\n    True\\n    >>> data = [-1.1, -1.5, -3.4, 2.5, 3.6, -3.3]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    >>> data = [9, 2, 7, 1, 5]\\n    >>> bucket_sort(data) == sorted(data)\\n    True\\n    \"\"\"\\n\\n    if len(my_list) == 0 or bucket_count <= 0:\\n        return []\\n\\n    min_value, max_value = min(my_list), max(my_list)\\n    if min_value == max_value:\\n        return my_list\\n\\n    bucket_size = (max_value - min_value) / bucket_count\\n    buckets: list[list] = [[] for _ in range(bucket_count)]\\n\\n    for val in my_list:\\n        index = min(int((val - min_value) / bucket_size), bucket_count - 1)\\n        buckets[index].append(val)\\n\\n    return [val for bucket in buckets for val in sorted(bucket)]'\n"
     ]
    }
   ],
   "source": [
    "print(chunks[77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE2rHcx8_ej0",
    "outputId": "59af7069-6676-4710-da3e-41ebbd070f02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlatIP: [[0.4405725 0.4405725 0.4405725 0.4405725 0.4405725]] [[14 13 12 11 10]]\n"
     ]
    }
   ],
   "source": [
    "ip_index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "ip_index.add(embeddings.copy())  # copy to avoid any accidental mutation\n",
    "D0, I0 = ip_index.search(search_embed, k=5)\n",
    "print(\"FlatIP:\", D0, I0)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "code-plagiarism-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
